"""
è‡ªé€‚åº”TVAæ¨¡å‹æµ‹è¯•è„šæœ¬

ç”¨äºéªŒè¯æ¨¡å‹çš„å‰å‘ä¼ æ’­æ˜¯å¦æ­£å¸¸å·¥ä½œï¼Œç‰¹åˆ«æ˜¯åŠ¨æ€Queryç”Ÿæˆçš„ä¿®å¤ã€‚
"""

import sys
import os
path = os.path.dirname(os.path.abspath(__file__))
sys.path.append(path)

import torch
import config
from models.adaptive_tva_fusion import AdaptiveTVA_fusion, AdaptiveGuidanceModule


def test_adaptive_guidance_module():
    """æµ‹è¯•è‡ªé€‚åº”å¼•å¯¼æ¨¡å—"""
    print("ğŸ§ª æµ‹è¯•è‡ªé€‚åº”å¼•å¯¼æ¨¡å—...")
    
    # åˆ›å»ºAGM
    agm = AdaptiveGuidanceModule(feature_dim=768, hidden_dim=128)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    feature_dim = 768
    
    text_repr = torch.randn(batch_size, feature_dim)
    vision_repr = torch.randn(batch_size, feature_dim)
    audio_repr = torch.randn(batch_size, feature_dim)
    
    # å‰å‘ä¼ æ’­
    weights = agm(text_repr, vision_repr, audio_repr)
    
    print(f"âœ… AGMæµ‹è¯•é€šè¿‡")
    print(f"   è¾“å…¥å½¢çŠ¶: [{batch_size}, {feature_dim}]")
    print(f"   è¾“å‡ºæƒé‡å½¢çŠ¶: {weights.shape}")
    print(f"   æƒé‡å’Œ: {weights.sum(dim=1)}")  # åº”è¯¥éƒ½æ¥è¿‘1.0
    print(f"   ç¤ºä¾‹æƒé‡: {weights[0].tolist()}")
    print()


def test_dynamic_query_creation():
    """æµ‹è¯•åŠ¨æ€Queryåˆ›å»º"""
    print("ğŸ”§ æµ‹è¯•åŠ¨æ€Queryåˆ›å»º...")
    
    # åˆ›å»ºæ¨¡å‹
    model = AdaptiveTVA_fusion(config)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®ï¼ˆæ¨¡æ‹ŸçœŸå®çš„åºåˆ—é•¿åº¦ï¼‰
    batch_size = 2
    text_seq_len = 62    # æ–‡æœ¬åºåˆ—é•¿åº¦ï¼ˆå¯å˜ï¼‰
    vision_seq_len = 500 # è§†è§‰åºåˆ—é•¿åº¦ï¼ˆå›ºå®šï¼‰
    audio_seq_len = 500  # éŸ³é¢‘åºåˆ—é•¿åº¦ï¼ˆå›ºå®šï¼‰
    feature_dim = 768
    
    text_seq = torch.randn(text_seq_len, batch_size, feature_dim)
    vision_seq = torch.randn(vision_seq_len, batch_size, feature_dim)
    audio_seq = torch.randn(audio_seq_len, batch_size, feature_dim)
    weights = torch.tensor([[0.6, 0.2, 0.2], [0.3, 0.4, 0.3]])  # æ¨¡æ‹Ÿæƒé‡
    
    # æµ‹è¯•åŠ¨æ€Queryåˆ›å»º
    try:
        dynamic_query = model._create_dynamic_query(text_seq, vision_seq, audio_seq, weights)
        print(f"âœ… åŠ¨æ€Queryåˆ›å»ºæˆåŠŸ")
        print(f"   æ–‡æœ¬åºåˆ—: [{text_seq_len}, {batch_size}, {feature_dim}]")
        print(f"   è§†è§‰åºåˆ—: [{vision_seq_len}, {batch_size}, {feature_dim}]")
        print(f"   éŸ³é¢‘åºåˆ—: [{audio_seq_len}, {batch_size}, {feature_dim}]")
        print(f"   åŠ¨æ€Query: {dynamic_query.shape}")
        print(f"   æƒé‡: {weights}")
    except Exception as e:
        print(f"âŒ åŠ¨æ€Queryåˆ›å»ºå¤±è´¥: {e}")
    
    print()


def test_model_forward():
    """æµ‹è¯•æ¨¡å‹å®Œæ•´å‰å‘ä¼ æ’­"""
    print("ğŸš€ æµ‹è¯•æ¨¡å‹å®Œæ•´å‰å‘ä¼ æ’­...")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cpu')  # ä½¿ç”¨CPUè¿›è¡Œæµ‹è¯•
    
    # åˆ›å»ºæ¨¡å‹
    model = AdaptiveTVA_fusion(config).to(device)
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    
    # æ–‡æœ¬æ•°æ®ï¼ˆå­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
    text = ["This is a test sentence.", "Another test sentence for validation."]
    
    # è§†è§‰æ•°æ®
    vision = torch.randn(batch_size, 500, 35).to(device)
    
    # éŸ³é¢‘æ•°æ®
    audio = torch.randn(batch_size, 500, 74).to(device)
    
    try:
        # å‰å‘ä¼ æ’­ï¼ˆè¯„ä¼°æ¨¡å¼ï¼‰
        with torch.no_grad():
            pred, guidance_weights = model(text, vision, audio, mode='eval')
        
        print(f"âœ… æ¨¡å‹å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   é¢„æµ‹å½¢çŠ¶: {pred.shape}")
        print(f"   é¢„æµ‹å€¼: {pred.tolist()}")
        print(f"   å¼•å¯¼æƒé‡å½¢çŠ¶: {guidance_weights.shape}")
        print(f"   å¼•å¯¼æƒé‡:")
        for i, weights in enumerate(guidance_weights):
            print(f"     æ ·æœ¬{i+1}: Text={weights[0]:.3f}, Vision={weights[1]:.3f}, Audio={weights[2]:.3f}")
        
        # éªŒè¯æƒé‡å’Œä¸º1
        weight_sums = guidance_weights.sum(dim=1)
        print(f"   æƒé‡å’ŒéªŒè¯: {weight_sums.tolist()} (åº”è¯¥éƒ½æ¥è¿‘1.0)")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print()


def test_training_mode():
    """æµ‹è¯•è®­ç»ƒæ¨¡å¼"""
    print("ğŸ¯ æµ‹è¯•è®­ç»ƒæ¨¡å¼...")
    
    device = torch.device('cpu')
    model = AdaptiveTVA_fusion(config).to(device)
    model.train()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    text = ["Training test sentence.", "Another training sentence."]
    vision = torch.randn(batch_size, 500, 35).to(device)
    audio = torch.randn(batch_size, 500, 74).to(device)
    
    try:
        # æ³¨æ„ï¼šè®­ç»ƒæ¨¡å¼éœ€è¦åŠ è½½å†»ç»“çš„ç¼–ç å™¨ï¼Œè¿™é‡Œæˆ‘ä»¬è·³è¿‡
        print("âš ï¸  è®­ç»ƒæ¨¡å¼éœ€è¦é¢„è®­ç»ƒçš„å†»ç»“ç¼–ç å™¨ï¼Œè·³è¿‡æ­¤æµ‹è¯•")
        print("   åœ¨å®é™…è®­ç»ƒä¸­ï¼Œè¯·ç¡®ä¿è°ƒç”¨ model.load_froze() åŠ è½½é¢„è®­ç»ƒæƒé‡")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
    
    print()


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("="*60)
    print("ğŸ§ª è‡ªé€‚åº”TVAæ¨¡å‹æµ‹è¯•")
    print("="*60)
    print("æµ‹è¯•ç›®æ ‡ï¼šéªŒè¯åºåˆ—é•¿åº¦ä¸åŒ¹é…é—®é¢˜çš„ä¿®å¤")
    print("="*60)
    
    # æµ‹è¯•å„ä¸ªç»„ä»¶
    test_adaptive_guidance_module()
    test_dynamic_query_creation()
    test_model_forward()
    test_training_mode()
    
    print("="*60)
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("="*60)
    
    print("\nğŸ“ æµ‹è¯•æ€»ç»“:")
    print("1. âœ… è‡ªé€‚åº”å¼•å¯¼æ¨¡å—å·¥ä½œæ­£å¸¸")
    print("2. âœ… åŠ¨æ€Queryåˆ›å»ºå¤„ç†äº†åºåˆ—é•¿åº¦ä¸åŒ¹é…é—®é¢˜")
    print("3. âœ… æ¨¡å‹å‰å‘ä¼ æ’­åœ¨è¯„ä¼°æ¨¡å¼ä¸‹æ­£å¸¸å·¥ä½œ")
    print("4. âš ï¸  è®­ç»ƒæ¨¡å¼éœ€è¦é¢„è®­ç»ƒçš„å†»ç»“ç¼–ç å™¨")
    
    print("\nğŸš€ ä¸‹ä¸€æ­¥:")
    print("1. ç¡®ä¿é¢„è®­ç»ƒçš„å•æ¨¡æ€ç¼–ç å™¨å·²è®­ç»ƒå®Œæˆ")
    print("2. è¿è¡Œå®Œæ•´çš„è‡ªé€‚åº”TVAè®­ç»ƒæµç¨‹")
    print("3. è§‚å¯Ÿæ—©åœæœºåˆ¶å’Œå¼•å¯¼æƒé‡åˆ†å¸ƒ")


if __name__ == '__main__':
    main()
