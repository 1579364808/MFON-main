import torch
import torch.nn.functional as F
from torch import nn
import numpy as np

import os
import sys
path = os.path.dirname(os.path.abspath(__file__))
sys.path.append(path)
sys.path.append(os.path.dirname(path))

from classifier import BaseClassifier
from Text_encoder import TextEncoder
from Vision_encoder import VisionEncoder
from Audio_encoder import AudioEncoder
from models.trans.transformer import TransformerEncoder
from models.classifier import BaseClassifier


def check_dir(path):
    """
    æ£€æŸ¥å¹¶åˆ›å»ºç›®å½•
    
    Args:
        path: ç›®å½•è·¯å¾„
    """
    if not os.path.exists(path):
        os.makedirs(path)


class AdaptiveGuidanceModule(nn.Module):
    """
    è‡ªé€‚åº”å¼•å¯¼æ¨¡å— (Adaptive Guidance Module, AGM)

    æ ¸å¿ƒåˆ›æ–°ï¼šä»"å›ºå®šå‘å¯¼"åˆ°"åŠ¨æ€é€‰ä¸¾"çš„è‡ªé€‚åº”å¼•å¯¼æœºåˆ¶

    è®¾è®¡ç†å¿µï¼š
    - ä¼ ç»Ÿæ–¹æ³•ï¼šæ–‡æœ¬æ°¸è¿œä½œä¸ºQueryï¼ˆå›ºå®šå‘å¯¼ï¼‰
    - åˆ›æ–°æ–¹æ³•ï¼šæ ¹æ®è¾“å…¥åŠ¨æ€é€‰ä¸¾æœ€é€‚åˆçš„å‘å¯¼æ¨¡æ€

    çµæ„Ÿæ¥æºï¼š
    1. Mixture of Experts (MoE): é—¨æ§ç½‘ç»œåŠ¨æ€é€‰æ‹©ä¸“å®¶
    2. GRUé—¨æ§æœºåˆ¶: åŸºäºè¾“å…¥åŠ¨æ€æ§åˆ¶ä¿¡æ¯æµ
    3. DeepSeekæ— æŸå¹³è¡¡: è§£å†³æ¨¡æ€è´Ÿè½½ä¸å‡è¡¡é—®é¢˜

    å·¥ä½œæµç¨‹ï¼š
    1. æ¥æ”¶ä¸‰ä¸ªæ¨¡æ€çš„å¥å­çº§è¡¨ç¤º
    2. é€šè¿‡é—¨æ§ç½‘ç»œè®¡ç®—æ¯ä¸ªæ¨¡æ€çš„"å‘å¯¼ç½®ä¿¡åº¦"
    3. åº”ç”¨æ— æŸå¹³è¡¡æœºåˆ¶ï¼ŒåŠ¨æ€è°ƒæ•´æ¨¡æ€åç½®
    4. è¾“å‡ºå¹³è¡¡çš„æƒé‡åˆ†å¸ƒ [Ï‰_t, Ï‰_v, Ï‰_a]

    ğŸ†• æ— æŸå¹³è¡¡æœºåˆ¶ï¼š
    - å¼•å…¥æ¨¡æ€åç½® (modality bias)ï¼ŒåŠ¨æ€è°ƒæ•´æ¨¡æ€é€‰æ‹©
    - åŸºäºå†å²è´Ÿè½½ç»Ÿè®¡ï¼Œè‡ªåŠ¨å¹³è¡¡æ¨¡æ€ä½¿ç”¨
    - ä¸å½±å“æ¢¯åº¦è®¡ç®—ï¼Œä¿æŒè®­ç»ƒç¨³å®šæ€§
    """

    def __init__(self, feature_dim, hidden_dim=128, dropout_rate=0.1, activation='relu',
                 enable_load_balancing=True, update_rate=0.01, target_balance_ratio=0.2):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”å¼•å¯¼æ¨¡å—

        Args:
            feature_dim: è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆé€šå¸¸æ˜¯encoder_fea_dimï¼Œå¦‚768ï¼‰
            hidden_dim: é—¨æ§ç½‘ç»œçš„éšè—å±‚ç»´åº¦
            dropout_rate: dropoutç‡
            activation: æ¿€æ´»å‡½æ•°ç±»å‹
            enable_load_balancing: æ˜¯å¦å¯ç”¨æ— æŸè´Ÿè½½å‡è¡¡
            update_rate: åç½®æ›´æ–°ç‡ (ç±»ä¼¼DeepSeekçš„uå‚æ•°)
            target_balance_ratio: ç›®æ ‡å¹³è¡¡æ¯”ä¾‹ (éä¸»å¯¼æ¨¡æ€çš„æœ€å°æƒé‡æ¯”ä¾‹)
        """
        super(AdaptiveGuidanceModule, self).__init__()

        # é€‰æ‹©æ¿€æ´»å‡½æ•°
        if activation == 'relu':
            act_fn = nn.ReLU()
        elif activation == 'gelu':
            act_fn = nn.GELU()
        elif activation == 'tanh':
            act_fn = nn.Tanh()
        else:
            act_fn = nn.ReLU()  # é»˜è®¤ä½¿ç”¨ReLU

        # é—¨æ§ç½‘ç»œï¼šè¾“å…¥ä¸‰ä¸ªæ¨¡æ€çš„æ‹¼æ¥ç‰¹å¾ï¼Œè¾“å‡ºä¸‰ä¸ªlogits (ä¸ä½¿ç”¨Softmax)
        self.gating_network = nn.Sequential(
            nn.Linear(feature_dim * 3, hidden_dim),  # è¾“å…¥ï¼š[bs, 768*3]
            act_fn,
            nn.Dropout(dropout_rate),  # å¯é…ç½®çš„dropoutç‡
            nn.Linear(hidden_dim, hidden_dim // 2),
            act_fn,
            nn.Linear(hidden_dim // 2, 3),  # è¾“å‡ºï¼š[bs, 3] logits
            # æ³¨æ„ï¼šç§»é™¤Softmaxï¼Œåœ¨forwardä¸­æ‰‹åŠ¨å¤„ç†
        )

        # ğŸ†• æ— æŸè´Ÿè½½å‡è¡¡å‚æ•°
        self.enable_load_balancing = enable_load_balancing
        self.update_rate = update_rate
        self.target_balance_ratio = target_balance_ratio

        # æ¨¡æ€åç½® (ç±»ä¼¼DeepSeekçš„expert bias)
        # ä½¿ç”¨register_bufferä½¿å…¶ä¸å‚ä¸æ¢¯åº¦è®¡ç®—ï¼Œä½†ä¼šè¢«ä¿å­˜å’ŒåŠ è½½
        self.register_buffer('modality_bias', torch.zeros(3))  # [text, vision, audio]

        # è´Ÿè½½ç»Ÿè®¡ (ç”¨äºåŠ¨æ€è°ƒæ•´åç½®)
        self.register_buffer('load_history', torch.zeros(3))  # å†å²è´Ÿè½½ç»Ÿè®¡
        self.register_buffer('update_count', torch.tensor(0))  # æ›´æ–°æ¬¡æ•°

    def forward(self, text_repr, vision_repr, audio_repr):
        """
        åŠ¨æ€é€‰ä¸¾å‘å¯¼æ¨¡æ€ (é›†æˆæ— æŸè´Ÿè½½å‡è¡¡)

        Args:
            text_repr: æ–‡æœ¬å¥å­çº§è¡¨ç¤º [batch_size, feature_dim]
            vision_repr: è§†è§‰å¥å­çº§è¡¨ç¤º [batch_size, feature_dim]
            audio_repr: éŸ³é¢‘å¥å­çº§è¡¨ç¤º [batch_size, feature_dim]

        Returns:
            weights: ä¸‰ä¸ªæ¨¡æ€çš„å‘å¯¼æƒé‡ [batch_size, 3]
                    weights[:, 0] = Ï‰_t (æ–‡æœ¬æƒé‡)
                    weights[:, 1] = Ï‰_v (è§†è§‰æƒé‡)
                    weights[:, 2] = Ï‰_a (éŸ³é¢‘æƒé‡)
        """
        # æ‹¼æ¥ä¸‰ä¸ªæ¨¡æ€çš„å¥å­çº§è¡¨ç¤º
        combined_features = torch.cat([text_repr, vision_repr, audio_repr], dim=-1)  # [bs, 768*3]

        # é€šè¿‡é—¨æ§ç½‘ç»œè®¡ç®—åŸå§‹logits
        raw_logits = self.gating_network(combined_features)  # [bs, 3]

        if self.enable_load_balancing and self.training:
            # ğŸ†• åº”ç”¨æ— æŸè´Ÿè½½å‡è¡¡
            balanced_weights = self._apply_load_balancing(raw_logits)

            # æ›´æ–°è´Ÿè½½ç»Ÿè®¡ (ç”¨äºä¸‹æ¬¡è°ƒæ•´åç½®)
            self._update_load_statistics(balanced_weights)

            return balanced_weights
        else:
            # æ¨ç†æ¨¡å¼æˆ–æœªå¯ç”¨è´Ÿè½½å‡è¡¡ï¼šç›´æ¥ä½¿ç”¨Softmax
            weights = torch.softmax(raw_logits, dim=-1)
            return weights

    def _apply_load_balancing(self, raw_logits):
        """
        åº”ç”¨æ— æŸè´Ÿè½½å‡è¡¡æœºåˆ¶

        æ ¸å¿ƒæ€æƒ³ï¼š
        1. åœ¨logitsä¸Šæ·»åŠ æ¨¡æ€åç½® (ä»…ç”¨äºæƒé‡è®¡ç®—)
        2. åç½®ä¸å‚ä¸æ¢¯åº¦è®¡ç®—ï¼Œé¿å…å¹²æ‰°ä¸»ä»»åŠ¡
        3. åŠ¨æ€è°ƒæ•´åç½®ä»¥å¹³è¡¡æ¨¡æ€ä½¿ç”¨

        Args:
            raw_logits: åŸå§‹é—¨æ§logits [batch_size, 3]

        Returns:
            balanced_weights: å¹³è¡¡åçš„æƒé‡ [batch_size, 3]
        """
        # æ·»åŠ æ¨¡æ€åç½®åˆ°logits (ç±»ä¼¼DeepSeekçš„ s_{i,t} + b_i)
        # æ³¨æ„ï¼šä½¿ç”¨detach()ç¡®ä¿åç½®ä¸å‚ä¸æ¢¯åº¦è®¡ç®—
        biased_logits = raw_logits + self.modality_bias.detach().unsqueeze(0)  # [bs, 3]

        # åŸºäºåç½®åçš„logitsè®¡ç®—æƒé‡
        balanced_weights = torch.softmax(biased_logits, dim=-1)  # [bs, 3]

        return balanced_weights

    def _update_load_statistics(self, weights):
        """
        æ›´æ–°è´Ÿè½½ç»Ÿè®¡å¹¶è°ƒæ•´æ¨¡æ€åç½®

        åŸºäºDeepSeekç®—æ³•1çš„é€‚é…ç‰ˆæœ¬ï¼š
        1. ç»Ÿè®¡å½“å‰batchä¸­æ¯ä¸ªæ¨¡æ€çš„å¹³å‡è´Ÿè½½
        2. è®¡ç®—è´Ÿè½½ä¸å¹³è¡¡è¯¯å·®
        3. æ›´æ–°æ¨¡æ€åç½®ä»¥ä¿ƒè¿›å¹³è¡¡

        Args:
            weights: å½“å‰batchçš„æƒé‡åˆ†å¸ƒ [batch_size, 3]
        """
        with torch.no_grad():  # ç¡®ä¿ä¸å½±å“æ¢¯åº¦
            # è®¡ç®—å½“å‰batchçš„æ¨¡æ€è´Ÿè½½ (å¹³å‡æƒé‡)
            current_load = weights.mean(dim=0)  # [3] - æ¯ä¸ªæ¨¡æ€çš„å¹³å‡æƒé‡

            # è®¡ç®—ç›®æ ‡è´Ÿè½½ (ç†æƒ³æƒ…å†µä¸‹åº”è¯¥ç›¸å¯¹å¹³è¡¡)
            # å…è®¸ä¸»å¯¼æ¨¡æ€æœ‰æ›´é«˜æƒé‡ï¼Œä½†é˜²æ­¢å…¶ä»–æ¨¡æ€å®Œå…¨è¢«å¿½ç•¥
            target_load = torch.tensor([
                1.0 - 2 * self.target_balance_ratio,  # ä¸»å¯¼æ¨¡æ€ç›®æ ‡æƒé‡
                self.target_balance_ratio,            # éä¸»å¯¼æ¨¡æ€ç›®æ ‡æƒé‡
                self.target_balance_ratio             # éä¸»å¯¼æ¨¡æ€ç›®æ ‡æƒé‡
            ], device=weights.device)

            # è®¡ç®—è´Ÿè½½ä¸å¹³è¡¡è¯¯å·®
            load_error = current_load - target_load  # [3]

            # æ›´æ–°æ¨¡æ€åç½® (ç±»ä¼¼DeepSeekçš„åç½®æ›´æ–°)
            # è´Ÿè½½è¿‡é«˜çš„æ¨¡æ€ï¼šé™ä½åç½® (å‡å°‘è¢«é€‰ä¸­æ¦‚ç‡)
            # è´Ÿè½½è¿‡ä½çš„æ¨¡æ€ï¼šæé«˜åç½® (å¢åŠ è¢«é€‰ä¸­æ¦‚ç‡)
            bias_update = -self.update_rate * torch.sign(load_error)

            # åº”ç”¨åç½®æ›´æ–°
            self.modality_bias += bias_update

            # æ›´æ–°å†å²ç»Ÿè®¡
            self.load_history = 0.9 * self.load_history + 0.1 * current_load
            self.update_count += 1

    def get_load_statistics(self):
        """
        è·å–è´Ÿè½½ç»Ÿè®¡ä¿¡æ¯ (ç”¨äºç›‘æ§å’Œè°ƒè¯•)

        Returns:
            dict: åŒ…å«è´Ÿè½½ç»Ÿè®¡çš„å­—å…¸
        """
        return {
            'modality_bias': self.modality_bias.cpu().numpy(),
            'load_history': self.load_history.cpu().numpy(),
            'update_count': self.update_count.item(),
            'target_balance_ratio': self.target_balance_ratio
        }


class AdaptiveTVA_fusion(nn.Module):
    """
    è‡ªé€‚åº”TVAå¤šæ¨¡æ€èåˆæ¨¡å‹
    
    æ ¸å¿ƒåˆ›æ–°ï¼šé›†æˆè‡ªé€‚åº”å¼•å¯¼æ¨¡å—(AGM)çš„å¤šæ¨¡æ€èåˆæ¶æ„
    
    ä¸åŸå§‹TVA_fusionçš„åŒºåˆ«ï¼š
    1. æ–°å¢AdaptiveGuidanceModuleï¼šåŠ¨æ€é€‰ä¸¾å‘å¯¼æ¨¡æ€
    2. åŠ¨æ€Queryç”Ÿæˆï¼šä¸å†å›ºå®šä½¿ç”¨æ–‡æœ¬ä½œä¸ºQuery
    3. è‡ªé€‚åº”è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼šæ ¹æ®æƒé‡è°ƒæ•´æ³¨æ„åŠ›è®¡ç®—
    
    ä¼˜åŠ¿ï¼š
    - å¤„ç†æç«¯æƒ…å†µï¼šå½“è¯­æ°”/è¡¨æƒ…æåº¦å¤¸å¼ è€Œæ–‡å­—å¹³æ·¡æ—¶
    - æé«˜é²æ£’æ€§ï¼šé€‚åº”ä¸åŒæ ·æœ¬çš„æ¨¡æ€é‡è¦æ€§åˆ†å¸ƒ
    - ä¿æŒå…¼å®¹æ€§ï¼šåœ¨æ–‡æœ¬ä¸»å¯¼çš„æƒ…å†µä¸‹é€€åŒ–ä¸ºåŸå§‹è¡Œä¸º
    """
    
    def __init__(self, config, agm_hidden_dim=None, agm_dropout=None, agm_activation=None,
                 enable_load_balancing=None, update_rate=None, target_balance_ratio=None):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”TVAèåˆæ¨¡å‹

        Args:
            config: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰è¶…å‚æ•°è®¾ç½®
            agm_hidden_dim: AGMéšè—å±‚ç»´åº¦ï¼ŒNoneæ—¶ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
            agm_dropout: AGM dropoutç‡ï¼ŒNoneæ—¶ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
            agm_activation: AGMæ¿€æ´»å‡½æ•°ï¼ŒNoneæ—¶ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
            enable_load_balancing: æ˜¯å¦å¯ç”¨è´Ÿè½½å‡è¡¡ï¼ŒNoneæ—¶ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
            update_rate: åç½®æ›´æ–°ç‡ï¼ŒNoneæ—¶ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
            target_balance_ratio: ç›®æ ‡å¹³è¡¡æ¯”ä¾‹ï¼ŒNoneæ—¶ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
        """
        super(AdaptiveTVA_fusion, self).__init__()
        self.config = config

        # è·å–è‡ªé€‚åº”é…ç½®
        adaptive_config = config.MOSEI.downStream.adaptiveTVAtrain

        # AGMé…ç½®å‚æ•°ï¼ˆæ”¯æŒå¤–éƒ¨ä¼ å…¥è¦†ç›–ï¼‰
        self.agm_hidden_dim = agm_hidden_dim or adaptive_config.agm_hidden_dim
        self.agm_dropout = agm_dropout or adaptive_config.agm_dropout
        self.agm_activation = agm_activation or adaptive_config.agm_activation

        # ğŸ†• è´Ÿè½½å‡è¡¡é…ç½®å‚æ•°
        self.enable_load_balancing = (enable_load_balancing if enable_load_balancing is not None
                                    else getattr(adaptive_config, 'enable_load_balancing', True))
        self.update_rate = (update_rate if update_rate is not None
                          else getattr(adaptive_config, 'load_balance_update_rate', 0.01))
        self.target_balance_ratio = (target_balance_ratio if target_balance_ratio is not None
                                   else getattr(adaptive_config, 'target_balance_ratio', 0.2))
        
        # ========== åŸºç¡€é…ç½®å‚æ•° ==========
        self.text_dropout = config.MOSEI.downStream.text_drop_out  # 0.3
        
        encoder_fea_dim = config.MOSEI.downStream.encoder_fea_dim  # 768
        
        # è·¨æ¨¡æ€Transformeré…ç½®
        audio_text_nhead = config.MOSEI.downStream.audio_text_nhead              # 8
        audio_text_tf_num_layers = config.MOSEI.downStream.audio_text_tf_num_layers  # 3
        vision_text_nhead = config.MOSEI.downStream.vision_text_nhead              # 8
        vision_text_tf_num_layers = config.MOSEI.downStream.vision_text_tf_num_layers  # 2
        
        attn_dropout = config.MOSEI.downStream.drop_out  # 0.1
        attn_mask = config.MOSEI.downStream.attn_mask    # True
        
        # å„æ¨¡æ€ç‰¹å¾ç»´åº¦
        audio_fea_dim = config.MOSEI.downStream.audio_fea_dim    # 74
        vision_fea_dim = config.MOSEI.downStream.vision_fea_dim  # 35
        text_fea_dim = config.MOSEI.downStream.text_fea_dim      # 768
        
        # åºåˆ—é•¿åº¦
        self.vlen, self.alen = config.MOSEI.downStream.vlen, config.MOSEI.downStream.alen  # 500, 500
        
        # ========== æ ¸å¿ƒåˆ›æ–°ï¼šè‡ªé€‚åº”å¼•å¯¼æ¨¡å— (é›†æˆæ— æŸè´Ÿè½½å‡è¡¡) ==========
        self.adaptive_guidance = AdaptiveGuidanceModule(
            feature_dim=encoder_fea_dim,                    # 768
            hidden_dim=self.agm_hidden_dim,                 # ä½¿ç”¨é…ç½®ä¸­çš„éšè—å±‚ç»´åº¦
            dropout_rate=self.agm_dropout,                  # ä½¿ç”¨é…ç½®ä¸­çš„dropoutç‡
            activation=self.agm_activation,                 # ä½¿ç”¨é…ç½®ä¸­çš„æ¿€æ´»å‡½æ•°
            enable_load_balancing=self.enable_load_balancing, # ğŸ†• å¯ç”¨è´Ÿè½½å‡è¡¡
            update_rate=self.update_rate,                   # ğŸ†• åç½®æ›´æ–°ç‡
            target_balance_ratio=self.target_balance_ratio  # ğŸ†• ç›®æ ‡å¹³è¡¡æ¯”ä¾‹
        )
        
        # ========== å¯å­¦ä¹ æç¤ºç¬¦ ==========
        self.use_learnable_vectors = config.MOSEI.downStream.use_learnable_vectors

        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åˆå§‹åŒ–å¯å­¦ä¹ å‘é‡
        if self.use_learnable_vectors:
            self.prompta_m = nn.Parameter(torch.rand(self.alen, encoder_fea_dim))  # [500, 768]
            self.promptv_m = nn.Parameter(torch.rand(self.vlen, encoder_fea_dim))  # [500, 768]
            print(f"âœ… [AdaptiveTVA] ä½¿ç”¨å¯å­¦ä¹ å‘é‡: éŸ³é¢‘å‘é‡ {self.prompta_m.shape}, è§†è§‰å‘é‡ {self.promptv_m.shape}")
        else:
            self.prompta_m = None
            self.promptv_m = None
            print("âŒ [AdaptiveTVA] ä¸ä½¿ç”¨å¯å­¦ä¹ å‘é‡")
        
        # ========== æ–‡æœ¬å¤„ç†æ¨¡å— ==========
        self.text_encoder = TextEncoder(config=config)
        self.proj_t = nn.Linear(text_fea_dim, encoder_fea_dim)  # 768â†’768
        
        # ========== è§†è§‰å¤„ç†æ¨¡å— ==========
        self.proj_v = nn.Linear(vision_fea_dim, encoder_fea_dim)  # 35â†’768
        
        # è§†è§‰-æ–‡æœ¬è·¨æ¨¡æ€Transformerï¼ˆç°åœ¨æ¥å—åŠ¨æ€Queryï¼‰
        self.vision_with_text = TransformerEncoder(
            embed_dim=encoder_fea_dim,
            num_heads=vision_text_nhead,
            layers=vision_text_tf_num_layers,
            attn_dropout=attn_dropout,
            relu_dropout=attn_dropout,
            res_dropout=attn_dropout,
            embed_dropout=attn_dropout,
            attn_mask=attn_mask
        )
        
        # ========== éŸ³é¢‘å¤„ç†æ¨¡å— ==========
        self.proj_a = nn.Linear(audio_fea_dim, encoder_fea_dim)  # 74â†’768
        
        # éŸ³é¢‘-æ–‡æœ¬è·¨æ¨¡æ€Transformerï¼ˆç°åœ¨æ¥å—åŠ¨æ€Queryï¼‰
        self.audio_with_text = TransformerEncoder(
            embed_dim=encoder_fea_dim,
            num_heads=audio_text_nhead,
            layers=audio_text_tf_num_layers,
            attn_dropout=attn_dropout,
            relu_dropout=attn_dropout,
            res_dropout=attn_dropout,
            embed_dropout=attn_dropout,
            attn_mask=attn_mask
        )
        
        # ========== çŸ¥è¯†è’¸é¦æ¨¡å— ==========
        self.vision_encoder_froze = VisionEncoder(config=config)
        self.audio_encoder_froze = AudioEncoder(config=config)
        
        # ========== æœ€ç»ˆèåˆåˆ†ç±»å™¨ ==========
        self.TVA_decoder = BaseClassifier(
            input_size=encoder_fea_dim * 3,  # 2304
            hidden_size=[encoder_fea_dim, encoder_fea_dim//2, encoder_fea_dim//8],  # [768, 384, 96]
            output_size=1
        )
        
        # ========== è®­ç»ƒç›¸å…³é…ç½® ==========
        self.device = config.DEVICE
        self.criterion = nn.KLDivLoss(reduction='batchmean')
        self.model_path = config.MOSEI.path.model_path + str(config.seed) + '/'
        check_dir(self.model_path)
        
    def load_froze(self):
        """
        åŠ è½½å¹¶å†»ç»“é¢„è®­ç»ƒçš„å•æ¨¡æ€ç¼–ç å™¨
        """
        model_path = self.config.MOSEI.path.encoder_path + str(self.config.seed) + '/'
        
        self.audio_encoder_froze.load_state_dict(
            torch.load(model_path + 'best_loss_audio_encoder.pt', map_location=self.device)
        )
        self.vision_encoder_froze.load_state_dict(
            torch.load(model_path + 'best_loss_vision_encoder.pt', map_location=self.device)
        )
        
        self.audio_encoder_froze.set_froze()
        self.vision_encoder_froze.set_froze()

    def _get_sentence_representations(self, text_seq, vision_seq, audio_seq):
        """
        è·å–ä¸‰ä¸ªæ¨¡æ€çš„å¥å­çº§è¡¨ç¤ºï¼ˆç”¨äºé—¨æ§ç½‘ç»œï¼‰

        Args:
            text_seq: æ–‡æœ¬åºåˆ—è¡¨ç¤º [seq_len, batch_size, feature_dim]
            vision_seq: è§†è§‰åºåˆ—è¡¨ç¤º [seq_len, batch_size, feature_dim]
            audio_seq: éŸ³é¢‘åºåˆ—è¡¨ç¤º [seq_len, batch_size, feature_dim]

        Returns:
            text_repr: æ–‡æœ¬å¥å­çº§è¡¨ç¤º [batch_size, feature_dim]
            vision_repr: è§†è§‰å¥å­çº§è¡¨ç¤º [batch_size, feature_dim]
            audio_repr: éŸ³é¢‘å¥å­çº§è¡¨ç¤º [batch_size, feature_dim]
        """
        # å–ç¬¬ä¸€ä¸ªä½ç½®çš„è¡¨ç¤ºä½œä¸ºå¥å­çº§è¡¨ç¤ºï¼ˆé€šå¸¸æ˜¯[CLS]ä½ç½®ï¼‰
        text_repr = text_seq[0]      # [batch_size, feature_dim]
        vision_repr = vision_seq[0]  # [batch_size, feature_dim]
        audio_repr = audio_seq[0]    # [batch_size, feature_dim]

        return text_repr, vision_repr, audio_repr

    def _apply_adaptive_weights(self, text_repr, vision_repr, audio_repr, weights):
        """
        åº”ç”¨è‡ªé€‚åº”æƒé‡åˆ°å¥å­çº§è¡¨ç¤º

        è¿™æ˜¯æˆ‘ä»¬åˆ›æ–°çš„æ ¸å¿ƒï¼šåœ¨å¥å­çº§åˆ«åº”ç”¨åŠ¨æ€æƒé‡ï¼Œé¿å…åºåˆ—é•¿åº¦ä¸åŒ¹é…é—®é¢˜

        Args:
            text_repr: æ–‡æœ¬å¥å­çº§è¡¨ç¤º [batch_size, feature_dim]
            vision_repr: è§†è§‰å¥å­çº§è¡¨ç¤º [batch_size, feature_dim]
            audio_repr: éŸ³é¢‘å¥å­çº§è¡¨ç¤º [batch_size, feature_dim]
            weights: æ¨¡æ€æƒé‡ [batch_size, 3]

        Returns:
            adaptive_text: è‡ªé€‚åº”æ–‡æœ¬è¡¨ç¤º [batch_size, feature_dim]
            adaptive_vision: è‡ªé€‚åº”è§†è§‰è¡¨ç¤º [batch_size, feature_dim]
            adaptive_audio: è‡ªé€‚åº”éŸ³é¢‘è¡¨ç¤º [batch_size, feature_dim]
        """
        # æå–æƒé‡
        w_t = weights[:, 0].unsqueeze(1)  # [bs, 1]
        w_v = weights[:, 1].unsqueeze(1)  # [bs, 1]
        w_a = weights[:, 2].unsqueeze(1)  # [bs, 1]

        # åº”ç”¨è‡ªé€‚åº”æƒé‡ï¼šæ¯ä¸ªæ¨¡æ€çš„è¡¨ç¤ºéƒ½æ˜¯ä¸‰ä¸ªæ¨¡æ€çš„åŠ æƒç»„åˆ
        # æƒé‡å†³å®šäº†æ¯ä¸ªæ¨¡æ€åœ¨æœ€ç»ˆè¡¨ç¤ºä¸­çš„é‡è¦æ€§
        adaptive_text = text_repr * w_t + vision_repr * w_v + audio_repr * w_a
        adaptive_vision = vision_repr * w_v + text_repr * w_t + audio_repr * w_a
        adaptive_audio = audio_repr * w_a + text_repr * w_t + vision_repr * w_v

        return adaptive_text, adaptive_vision, adaptive_audio

    def forward(self, text, vision, audio, mode='train'):
        """
        è‡ªé€‚åº”TVAèåˆæ¨¡å‹çš„å‰å‘ä¼ æ’­

        æ ¸å¿ƒåˆ›æ–°ï¼šåŠ¨æ€é€‰ä¸¾å‘å¯¼æ¨¡æ€ï¼Œè€Œéå›ºå®šä½¿ç”¨æ–‡æœ¬

        Args:
            text: æ–‡æœ¬è¾“å…¥ï¼Œå­—ç¬¦ä¸²åˆ—è¡¨
            vision: è§†è§‰ç‰¹å¾ [batch_size, 500, 35]
            audio: éŸ³é¢‘ç‰¹å¾ [batch_size, 500, 74]
            mode: æ¨¡å¼ï¼Œ'train'æ—¶è®¡ç®—æ‰€æœ‰æŸå¤±ï¼Œ'eval'æ—¶åªè¿”å›é¢„æµ‹

        Returns:
            pred: æƒ…æ„Ÿé¢„æµ‹å€¼ [batch_size]
            losses: è®­ç»ƒæ¨¡å¼ä¸‹è¿”å› (loss_v, loss_a, loss_nce, guidance_weights)
                   æ¨ç†æ¨¡å¼ä¸‹è¿”å› None

        åˆ›æ–°æµç¨‹ï¼š
        1. è·å–åˆå§‹ç‰¹å¾è¡¨ç¤º
        2. åŠ¨æ€é€‰ä¸¾å‘å¯¼æ¨¡æ€ï¼ˆAGMæ ¸å¿ƒï¼‰
        3. æ ‡å‡†è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆæ–‡æœ¬ä½œä¸ºQueryï¼‰
        4. åœ¨å¥å­çº§åˆ«åº”ç”¨è‡ªé€‚åº”æƒé‡ï¼ˆé¿å…åºåˆ—é•¿åº¦ä¸åŒ¹é…ï¼‰
        5. ç‰¹å¾èåˆå’Œæƒ…æ„Ÿé¢„æµ‹
        6. æŸå¤±è®¡ç®—ï¼ˆåŒ…å«å¼•å¯¼æƒé‡ä¿¡æ¯ï¼‰

        å…³é”®åˆ›æ–°ï¼šåœ¨å¥å­çº§è¡¨ç¤ºä¸Šåº”ç”¨åŠ¨æ€æƒé‡ï¼Œè€Œéåºåˆ—çº§åˆ«ï¼Œ
        è¿™æ ·æ—¢å®ç°äº†è‡ªé€‚åº”å¼•å¯¼ï¼Œåˆé¿å…äº†åºåˆ—é•¿åº¦ä¸åŒ¹é…çš„é—®é¢˜ã€‚
        """

        # ========== æ­¥éª¤1: æ–‡æœ¬ç¼–ç å’Œå¤„ç† ==========
        last_hidden_text = self.text_encoder(text)  # [bs, seq_len, 768]

        # æ–‡æœ¬æŠ•å½±å’Œdropout
        last_hidden_text = F.dropout(
            self.proj_t(last_hidden_text.permute(1, 0, 2)),
            p=self.text_dropout,
            training=self.training
        )  # [seq, bs, 768]

        # ========== æ­¥éª¤2: è§†è§‰å’ŒéŸ³é¢‘åˆå§‹å¤„ç† ==========
        # è§†è§‰ç‰¹å¾æŠ•å½±å¹¶æ ¹æ®é…ç½®æ·»åŠ æç¤ºç¬¦
        proj_vision = self.proj_v(vision).permute(1, 0, 2)  # [bs, 500, 35] â†’ [500, bs, 768]
        if self.use_learnable_vectors and self.promptv_m is not None:
            proj_vision = proj_vision + self.promptv_m.unsqueeze(1)  # æ·»åŠ å¯å­¦ä¹ è§†è§‰å‘é‡

        # éŸ³é¢‘ç‰¹å¾æŠ•å½±å¹¶æ ¹æ®é…ç½®æ·»åŠ æç¤ºç¬¦
        proj_audio = self.proj_a(audio).permute(1, 0, 2)  # [bs, 500, 74] â†’ [500, bs, 768]
        if self.use_learnable_vectors and self.prompta_m is not None:
            proj_audio = proj_audio + self.prompta_m.unsqueeze(1)  # æ·»åŠ å¯å­¦ä¹ éŸ³é¢‘å‘é‡

        # ========== æ­¥éª¤3: æ ¸å¿ƒåˆ›æ–° - åŠ¨æ€é€‰ä¸¾å‘å¯¼æ¨¡æ€ ==========
        # è·å–ä¸‰ä¸ªæ¨¡æ€çš„å¥å­çº§è¡¨ç¤º
        text_repr, vision_repr, audio_repr = self._get_sentence_representations(
            last_hidden_text, proj_vision, proj_audio
        )

        # é€šè¿‡è‡ªé€‚åº”å¼•å¯¼æ¨¡å—è®¡ç®—æƒé‡
        guidance_weights = self.adaptive_guidance(text_repr, vision_repr, audio_repr)
        # [batch_size, 3] where [:, 0]=Ï‰_t, [:, 1]=Ï‰_v, [:, 2]=Ï‰_a

        # ========== æ­¥éª¤4: æ ‡å‡†è·¨æ¨¡æ€æ³¨æ„åŠ› ==========
        # ä½¿ç”¨æ–‡æœ¬ä½œä¸ºQueryè¿›è¡Œè·¨æ¨¡æ€æ³¨æ„åŠ›è®¡ç®—
        # æ³¨æ„ï¼šæˆ‘ä»¬åœ¨å¥å­çº§åˆ«åº”ç”¨è‡ªé€‚åº”æƒé‡ï¼Œè€Œä¸æ˜¯åœ¨è¿™é‡Œ

        # è§†è§‰-æ–‡æœ¬è·¨æ¨¡æ€æ³¨æ„åŠ›
        h_tv = self.vision_with_text(last_hidden_text, proj_vision, proj_vision)

        # éŸ³é¢‘-æ–‡æœ¬è·¨æ¨¡æ€æ³¨æ„åŠ›
        h_ta = self.audio_with_text(last_hidden_text, proj_audio, proj_audio)

        # ========== æ­¥éª¤5: æå–å¢å¼ºåçš„æ¨¡æ€è¡¨ç¤ºå¹¶åº”ç”¨è‡ªé€‚åº”æƒé‡ ==========
        # æå–åŸºç¡€çš„å¥å­çº§è¡¨ç¤º
        base_text_embed = last_hidden_text[0]  # åŸå§‹æ–‡æœ¬è¡¨ç¤º [bs, 768]
        base_vision_embed = h_tv[0]            # è§†è§‰å¢å¼ºè¡¨ç¤º [bs, 768]
        base_audio_embed = h_ta[0]             # éŸ³é¢‘å¢å¼ºè¡¨ç¤º [bs, 768]

        # åº”ç”¨è‡ªé€‚åº”æƒé‡ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
        x_t_embed, x_v_embed, x_a_embed = self._apply_adaptive_weights(
            base_text_embed, base_vision_embed, base_audio_embed, guidance_weights
        )

        # ========== æ­¥éª¤6: å¤šæ¨¡æ€ç‰¹å¾èåˆ ==========
        x = torch.cat([x_t_embed, x_v_embed, x_a_embed], dim=-1)  # [bs, 2304]
        pred = self.TVA_decoder(x).view(-1)  # [bs]

        # ========== æ­¥éª¤7: æŸå¤±è®¡ç®—ï¼ˆä»…è®­ç»ƒæ¨¡å¼ï¼‰ ==========
        if mode == 'train':
            # çŸ¥è¯†è’¸é¦æŸå¤±
            x_v_embed_froze = self.vision_encoder_froze(vision).squeeze()
            x_a_embed_froze = self.audio_encoder_froze(audio).squeeze()

            loss_v = self.get_KL_loss(x_v_embed, x_v_embed_froze)
            loss_a = self.get_KL_loss(x_a_embed, x_a_embed_froze)

            # å¯¹æ¯”å­¦ä¹ æŸå¤±
            loss_nce = (self.get_InfoNCE_loss(x_v_embed, x_t_embed) +
                       self.get_InfoNCE_loss(x_a_embed, x_t_embed))

            # è¿”å›æŸå¤±å’Œå¼•å¯¼æƒé‡ï¼ˆç”¨äºåˆ†æå’Œå¯è§†åŒ–ï¼‰
            return pred, (loss_v, loss_a, loss_nce, guidance_weights)
        else:
            return pred, guidance_weights  # æ¨ç†æ—¶ä¹Ÿè¿”å›æƒé‡ç”¨äºåˆ†æ

    def save_model(self, name=None):
        """
        ä¿å­˜è‡ªé€‚åº”TVAèåˆæ¨¡å‹

        Args:
            name: ä¿å­˜æ–‡ä»¶çš„å‰ç¼€åï¼Œé»˜è®¤ä¸ºNoneæ—¶ä½¿ç”¨é»˜è®¤å‘½å
        """
        if name == None:
            mode_path = self.model_path + 'AdaptiveTVA_fusion' + '_model.pt'
        else:
            mode_path = self.model_path + str(name) + 'AdaptiveTVA_fusion' + '_model.pt'

        print('Adaptive model saved at:\n', mode_path)
        torch.save(self.state_dict(), mode_path)

    def load_model(self, name=None):
        """
        åŠ è½½è‡ªé€‚åº”TVAèåˆæ¨¡å‹

        Args:
            name: åŠ è½½æ–‡ä»¶çš„è·¯å¾„æˆ–å‰ç¼€å
        """
        if name == None:
            mode_path = self.model_path + 'AdaptiveTVA_fusion' + '_model.pt'
        else:
            mode_path = name

        print('Adaptive model loaded from:\n', mode_path)

        checkpoint = torch.load(mode_path, map_location=self.device)
        model_state_dict = self.state_dict()
        filtered_checkpoint = {k: v for k, v in checkpoint.items() if k in model_state_dict}
        self.load_state_dict(filtered_checkpoint, strict=False)

    def get_KL_loss(self, x_embed, x_embed_target):
        """
        è®¡ç®—çŸ¥è¯†è’¸é¦çš„KLæ•£åº¦æŸå¤±
        """
        x_embed1 = F.log_softmax(x_embed, dim=1)
        x_embed_target1 = F.softmax(x_embed_target, dim=1)
        loss = self.criterion(x_embed1, x_embed_target1)
        return loss

    def get_InfoNCE_loss(self, input1, input2):
        """
        è®¡ç®—InfoNCEå¯¹æ¯”å­¦ä¹ æŸå¤±
        """
        x1 = input1 / input1.norm(dim=1, keepdim=True)
        x2 = input2 / input2.norm(dim=1, keepdim=True)

        pos = torch.sum(x1 * x2, dim=-1)
        neg = torch.logsumexp(torch.matmul(x1, x2.t()), dim=-1)
        nce_loss = -(pos - neg).mean()

        return nce_loss

    def analyze_guidance_weights(self, guidance_weights):
        """
        åˆ†æå¼•å¯¼æƒé‡çš„åˆ†å¸ƒæƒ…å†µ

        Args:
            guidance_weights: å¼•å¯¼æƒé‡ [batch_size, 3]

        Returns:
            analysis: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        with torch.no_grad():
            # è®¡ç®—æ¯ä¸ªæ¨¡æ€çš„å¹³å‡æƒé‡
            mean_weights = guidance_weights.mean(dim=0)  # [3]

            # è®¡ç®—æƒé‡çš„æ ‡å‡†å·®ï¼ˆè¡¡é‡é€‰æ‹©çš„å¤šæ ·æ€§ï¼‰
            std_weights = guidance_weights.std(dim=0)   # [3]

            # ç»Ÿè®¡ä¸»å¯¼æ¨¡æ€ï¼ˆæƒé‡æœ€å¤§çš„æ¨¡æ€ï¼‰
            dominant_modality = guidance_weights.argmax(dim=1)  # [batch_size]
            modality_counts = torch.bincount(dominant_modality, minlength=3)

            analysis = {
                'mean_text_weight': mean_weights[0].item(),
                'mean_vision_weight': mean_weights[1].item(),
                'mean_audio_weight': mean_weights[2].item(),
                'std_text_weight': std_weights[0].item(),
                'std_vision_weight': std_weights[1].item(),
                'std_audio_weight': std_weights[2].item(),
                'text_dominant_count': modality_counts[0].item(),
                'vision_dominant_count': modality_counts[1].item(),
                'audio_dominant_count': modality_counts[2].item(),
                'total_samples': guidance_weights.size(0)
            }

        return analysis
