import torch
import datetime
import os
from tqdm import tqdm

from utils import write_log, set_random_seed, write_config
from models.model import TVA_fusion
import torch.nn.functional as F

def focal_bce_with_logits(logits, targets, alpha=0.5, gamma=2.0):
    probs = torch.sigmoid(logits)
    ce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')
    p_t = probs * targets + (1 - probs) * (1 - targets)
    alpha_t = alpha * targets + (1 - alpha) * (1 - targets)
    return (alpha_t * (1 - p_t).pow(gamma) * ce).mean()



def TVA_train_fusion(config, metrics, seed, train_data, valid_data, test_data):
    print('---------------TVA_EXP---------------')

    set_random_seed(seed)

    update_epochs = config.SIMS.downStream.update_epochs

    text_lr = config.SIMS.downStream.TVAtrain.text_lr
    audio_lr = config.SIMS.downStream.TVAtrain.audio_lr
    vision_lr = config.SIMS.downStream.TVAtrain.vision_lr
    other_lr = config.SIMS.downStream.TVAtrain.other_lr

    text_decay = config.SIMS.downStream.TVAtrain.text_decay
    audio_decay = config.SIMS.downStream.TVAtrain.audio_decay
    vision_decay = config.SIMS.downStream.TVAtrain.vision_decay
    other_decay = config.SIMS.downStream.TVAtrain.other_decay

    # åˆ é™¤çŸ¥è¯†è’¸é¦å’Œå¯¹æ¯”å­¦ä¹ æƒé‡ - ä¸å†ä½¿ç”¨

    model = TVA_fusion(config).to(config.DEVICE)
    # åˆ é™¤load_frozeè°ƒç”¨ - ä¸å†éœ€è¦åŠ è½½å†»ç»“ç¼–ç å™¨

    text_params = list(model.proj_t.named_parameters()) + list(model.text_encoder.named_parameters())
    text_params = [p for _, p in text_params]
    vision_params = list(model.proj_v.named_parameters()) +\
                list(model.vision_with_text.named_parameters())
    vision_params = [p for _, p in vision_params]
    # åªæœ‰åœ¨ä½¿ç”¨å¯å­¦ä¹ å‘é‡æ—¶æ‰æ·»åŠ åˆ°ä¼˜åŒ–å™¨å‚æ•°ä¸­
    if model.promptv_m is not None:
        vision_params.append(model.promptv_m)

    audio_params = list(model.proj_a.named_parameters()) +\
                list(model.audio_with_text.named_parameters())
    audio_params = [p for _, p in audio_params]
    # åªæœ‰åœ¨ä½¿ç”¨å¯å­¦ä¹ å‘é‡æ—¶æ‰æ·»åŠ åˆ°ä¼˜åŒ–å™¨å‚æ•°ä¸­
    if model.prompta_m is not None:
        audio_params.append(model.prompta_m)
    model_params_other = [p for n, p in list(model.named_parameters()) if '_decoder' in n]

    # è¾…åŠ©åˆ†ç±»å¤´å‚æ•°
    aux_params = []
    if getattr(model, 'enable_aux_cls', False) and getattr(model, 'cls_head', None) is not None:
        aux_params = list(model.cls_head.parameters())

    optimizer_grouped_parameters = [
        {'params': text_params, 'weight_decay': text_decay, 'lr': text_lr},
        {'params': audio_params, 'weight_decay': audio_decay, 'lr': audio_lr},
        {'params': vision_params, 'weight_decay': vision_decay, 'lr': vision_lr},
        {'params': model_params_other, 'weight_decay': other_decay, 'lr': other_lr}
    ] + ([{'params': aux_params, 'weight_decay': other_decay, 'lr': other_lr}] if aux_params else [])
    optimizer = torch.optim.Adam(optimizer_grouped_parameters)

    loss, best_loss  = 0, 1e8
    pred_loss = 0  # åªä¿ç•™é¢„æµ‹æŸå¤±
    device = config.DEVICE
    total_epoch = config.SIMS.downStream.TVAtrain.epoch
    best_epoch = 1

    # ========== AtCAF K-means åˆå§‹åŒ–æ£€æŸ¥ ==========
    need_atcaf_init = config.SIMS.downStream.use_atcaf_plugin and not model.atcaf_plugin.vision_dict.initialized

    if need_atcaf_init:
        print("\nğŸ”„ æ£€æµ‹åˆ°æ··æ·†å› å­å­—å…¸æœªåˆå§‹åŒ–ï¼Œå¼€å§‹æ”¶é›†ç‰¹å¾...")
        model.train()

        feature_bar = tqdm(train_data, desc="æ”¶é›†ç‰¹å¾ç”¨äºK-meansåˆå§‹åŒ–", disable=False)
        for batch_data in feature_bar:
            text = batch_data['raw_text']
            vision = batch_data['vision'].clone().detach().to(device).float()
            audio = batch_data['audio'].clone().detach().to(device).float()
            with torch.no_grad():
                _ = model(text, vision, audio, mode='train')

        print("âœ… ç‰¹å¾æ”¶é›†å®Œæˆï¼Œå¼€å§‹K-meansèšç±»...")
        model.atcaf_plugin.initialize_kmeans_dictionaries()
        print("ğŸ¯ æ··æ·†å› å­å­—å…¸åˆå§‹åŒ–å®Œæˆï¼")

    # åˆå§‹åŒ–æŸå¤±ç»Ÿè®¡å˜é‡
    running_pred_loss = 0.0
    running_atcaf_loss = 0.0
    running_total_loss = 0.0
    batch_count = 0

    # ç”¨äºä¿å­˜æœ€ä½³æ¨¡å‹çš„å˜é‡
    best_valid_loss = float('inf')
    best_epoch = -1
    best_model_path = None

    for epoch in range(1, total_epoch + 1):
        print(f"\nğŸš€ å¼€å§‹ç¬¬ {epoch}/{total_epoch} ä¸ªEpoch")

        model.train()
        left_epochs = update_epochs
        bar = tqdm(train_data, disable=False)

        # é‡ç½®æ¯ä¸ªepochçš„æŸå¤±ç»Ÿè®¡
        running_pred_loss = 0.0
        running_atcaf_loss = 0.0
        running_atcaf_weighted_loss = 0.0  # æ–°å¢ï¼šåŠ æƒåçš„AtCAFæŸå¤±
        running_total_loss = 0.0
        batch_count = 0



        for batch_data in bar:
            try:
                # åŠ¨æ€æ„å»ºè¿›åº¦æ¡æè¿°ï¼Œåªæ˜¾ç¤ºå¯ç”¨çš„æŸå¤±é¡¹
                desc_parts = [f"Epoch:{epoch}", f"Total:[{loss.item():.4f}]", f"Pred:[{pred_loss.item():.4f}]"]

                # AtCAFæŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if getattr(model.config.SIMS.downStream, 'use_atcaf_plugin', False):
                    desc_parts.append(f"AtCAF(raw):[{atcaf_loss_raw.item():.6f}]")
                    atcaf_w_val = atcaf_loss_weighted.item() if 'atcaf_loss_weighted' in locals() and torch.is_tensor(atcaf_loss_weighted) else (atcaf_loss_weighted if 'atcaf_loss_weighted' in locals() else 0.0)
                    desc_parts.append(f"AtCAF(w):[{atcaf_w_val:.6f}]")

                # HCLæŸå¤±ï¼ˆåªæ˜¾ç¤ºå¯ç”¨çš„ï¼‰
                if getattr(model.config.SIMS.downStream, 'enable_hcl', False):
                    lambda1 = getattr(model.config.SIMS.downStream, 'lambda_scl', 0.0)
                    lambda2 = getattr(model.config.SIMS.downStream, 'lambda_iamcl', 0.0)
                    lambda3 = getattr(model.config.SIMS.downStream, 'lambda_iemcl', 0.0)

                    if lambda1 != 0:
                        scl_val = L_scl.item() if 'L_scl' in locals() and torch.is_tensor(L_scl) else 0.0
                        desc_parts.append(f"SCL:[{scl_val:.4f}]")
                    if lambda2 != 0:
                        iamcl_val = L_iamcl.item() if 'L_iamcl' in locals() and torch.is_tensor(L_iamcl) else 0.0
                        desc_parts.append(f"IAMCL:[{iamcl_val:.4f}]")
                    if lambda3 != 0:
                        iemcl_val = L_iemcl.item() if 'L_iemcl' in locals() and torch.is_tensor(L_iemcl) else 0.0
                        desc_parts.append(f"IEMCL:[{iemcl_val:.4f}]")

                    # å¦‚æœæœ‰ä»»ä½•HCLæŸå¤±å¯ç”¨ï¼Œæ˜¾ç¤ºæ··åˆæŸå¤±
                    if lambda1 != 0 or lambda2 != 0 or lambda3 != 0:
                        hybrid_val = L_hybrid.item() if 'L_hybrid' in locals() and torch.is_tensor(L_hybrid) else 0.0
                        desc_parts.append(f"Hybrid:[{hybrid_val:.4f}]")

                # Focal Lossï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if getattr(model.config.SIMS.downStream, 'enable_aux_cls', False):
                    focal_val = L_focal.item() if 'L_focal' in locals() and torch.is_tensor(L_focal) else 0.0
                    desc_parts.append(f"FLoss:[{focal_val:.4f}]")

                bar.set_description("|".join(desc_parts) + "|")
            except:
                bar.set_description("Epoch:%d|æŸå¤±è®¡ç®—ä¸­...|" % epoch)

            if left_epochs == update_epochs:
                optimizer.zero_grad()
            left_epochs -= 1

            text = batch_data['raw_text']
            vision = batch_data['vision'].clone().detach().to(device).float()
            audio = batch_data['audio'].clone().detach().to(device).float()
            label = batch_data['labels']['M'].clone().detach().to(device).float()

            if getattr(model.config.SIMS.downStream, 'enable_aux_cls', False):
                pred, atcaf_loss_raw, embeds, cls_logits = model(text, vision, audio, mode='train', labels=label)
            else:
                pred, atcaf_loss_raw, embeds = model(text, vision, audio, mode='train', labels=label)
                cls_logits = None
            x_t, x_v, x_a = embeds  # [B, D] each

            # è°ƒè¯•ï¼šæ£€æŸ¥åŸå§‹åµŒå…¥
            if batch_count < 3 and epoch == 1:
                print(f"\n[è°ƒè¯•] åŸå§‹åµŒå…¥å½¢çŠ¶: x_t={x_t.shape}, x_v={x_v.shape}, x_a={x_a.shape}")
                print(f"[è°ƒè¯•] åŸå§‹åµŒå…¥èŒƒæ•°: t={torch.norm(x_t, dim=1).mean().item():.4f}, v={torch.norm(x_v, dim=1).mean().item():.4f}, a={torch.norm(x_a, dim=1).mean().item():.4f}")
                print(f"[è°ƒè¯•] åµŒå…¥æ˜¯å¦éœ€è¦æ¢¯åº¦: t={x_t.requires_grad}, v={x_v.requires_grad}, a={x_a.requires_grad}")

            # L2-normalize embeddings before any contrastive losses
            from train.contrastive_losses import l2_normalize, compute_scl_loss, compute_iamcl_loss, compute_iemcl_loss
            x_t, x_v, x_a = l2_normalize(x_t, x_v, x_a)

            pred_loss = torch.mean((pred-label)*(pred-label))  # [bs]

            # åˆå§‹åŒ–å½“å‰æ‰¹æ¬¡çš„æ€»æŸå¤±
            loss = pred_loss
            atcaf_loss_weighted = torch.tensor(0.0, device=device, dtype=torch.float32) # åˆå§‹åŒ–åŠ æƒæŸå¤±

            # ===== æ··åˆå¯¹æ¯”æŸå¤± =====
            if getattr(model.config.SIMS.downStream, 'enable_hcl', False):
                alpha = getattr(model.config.SIMS.downStream, 'contrastive_alpha', 0.5)
                lambda1 = getattr(model.config.SIMS.downStream, 'lambda_scl', 0.0)
                lambda2 = getattr(model.config.SIMS.downStream, 'lambda_iamcl', 0.0)
                lambda3 = getattr(model.config.SIMS.downStream, 'lambda_iemcl', 0.0)

                # è½¬æ¢è¿ç»­æ ‡ç­¾ä¸ºäºŒå…ƒç±»åˆ«ï¼ˆ>0 æ­£ç±»ï¼Œå¦åˆ™è´Ÿç±»ï¼‰
                cls_labels = (label.view(-1) > 0).long()

                # è°ƒè¯•ä¿¡æ¯ï¼šæ¯100ä¸ªbatchæ‰“å°ä¸€æ¬¡ç±»åˆ«åˆ†å¸ƒå’Œç›¸ä¼¼åº¦ç»Ÿè®¡
                if batch_count % 100 == 0:
                    pos_count = (cls_labels == 1).sum().item()
                    neg_count = (cls_labels == 0).sum().item()

                    # è®¡ç®—ç›¸ä¼¼åº¦ç»Ÿè®¡
                    with torch.no_grad():
                        sim_tv = torch.sum(x_t * x_v, dim=1)
                        sim_ta = torch.sum(x_t * x_a, dim=1)
                        sim_va = torch.sum(x_v * x_a, dim=1)

                        print(f"\n[è°ƒè¯•] Batch {batch_count}:")
                        print(f"  ç±»åˆ«åˆ†å¸ƒ: æ­£ç±»={pos_count}, è´Ÿç±»={neg_count}, æ¯”ä¾‹={pos_count/(pos_count+neg_count):.3f}")
                        print(f"  ç›¸ä¼¼åº¦ç»Ÿè®¡ (alpha={alpha:.1f}):")
                        print(f"    sim_tv: mean={sim_tv.mean():.3f}, std={sim_tv.std():.3f}, range=[{sim_tv.min():.3f}, {sim_tv.max():.3f}]")
                        print(f"    sim_ta: mean={sim_ta.mean():.3f}, std={sim_ta.std():.3f}, range=[{sim_ta.min():.3f}, {sim_ta.max():.3f}]")
                        print(f"    sim_va: mean={sim_va.mean():.3f}, std={sim_va.std():.3f}, range=[{sim_va.min():.3f}, {sim_va.max():.3f}]")

                        # æ£€æŸ¥ç‰¹å¾æœ¬èº«çš„åˆ†å¸ƒ
                        print(f"  ç‰¹å¾èŒƒæ•°æ£€æŸ¥:")
                        print(f"    x_t norm: mean={torch.norm(x_t, dim=1).mean():.3f}")
                        print(f"    x_v norm: mean={torch.norm(x_v, dim=1).mean():.3f}")
                        print(f"    x_a norm: mean={torch.norm(x_a, dim=1).mean():.3f}")

                        # é¢„æœŸSCLæŸå¤±
                        expected_scl = ((sim_tv - alpha)**2 + (sim_ta - alpha)**2 + (sim_va - alpha)**2).mean() / 3.0
                        print(f"  é¢„æœŸSCLæŸå¤±: {expected_scl:.4f}")
            # ===== Focal Lossï¼ˆè¾…åŠ©åˆ†ç±»ï¼‰=====
            L_focal = torch.tensor(0.0, device=device)
            if cls_logits is not None:
                binary_labels = (label.view(-1) > 0).float()
                fa = getattr(model.config.SIMS.downStream, 'focal_alpha', None)
                if fa is None:
                    pos = (binary_labels == 1).sum().item()
                    neg = (binary_labels == 0).sum().item()
                    tot = max(pos + neg, 1)
                    alpha_pos = neg / tot
                else:
                    alpha_pos = float(fa)
                gamma = getattr(model.config.SIMS.downStream, 'focal_gamma', 2.0)
                focal_w = getattr(model.config.SIMS.downStream, 'focal_weight', 0.1)
                L_focal = focal_bce_with_logits(cls_logits, binary_labels, alpha=alpha_pos, gamma=gamma)
                loss = loss + focal_w * L_focal

                # è°ƒè¯•SCLï¼šå‰å‡ ä¸ªbatchæ‰“å°è¯¦ç»†ä¿¡æ¯
                debug_scl = (batch_count < 3 and epoch == 1)
                L_scl = compute_scl_loss(x_t, x_v, x_a, alpha, debug=debug_scl) if lambda1 != 0 else torch.tensor(0.0, device=device)
                L_iamcl = compute_iamcl_loss(x_t, x_v, x_a, cls_labels) if lambda2 != 0 else torch.tensor(0.0, device=device)
                L_iemcl = compute_iemcl_loss(x_t, x_v, x_a, cls_labels, alpha) if lambda3 != 0 else torch.tensor(0.0, device=device)

                L_hybrid = lambda1 * L_scl + lambda2 * L_iamcl + lambda3 * L_iemcl
                loss = loss + L_hybrid



            # æ·»åŠ AtCAFæŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if torch.is_tensor(atcaf_loss_raw) and atcaf_loss_raw.item() > 0:
                counterfactual_weight = model.config.SIMS.downStream.counterfactual_weight

                # warm-up: å‰ atcaf_warmup_epochs è½®ï¼Œå°†æƒé‡çº¿æ€§ä» 0 -> counterfactual_weight
                warmup_epochs = getattr(model.config.SIMS.downStream, 'atcaf_warmup_epochs', 0)
                if warmup_epochs and epoch <= warmup_epochs:
                    scale = float(epoch) / float(warmup_epochs)
                    eff_weight = counterfactual_weight * scale
                else:
                    eff_weight = counterfactual_weight

                atcaf_loss_weighted = eff_weight * atcaf_loss_raw
                loss = loss + atcaf_loss_weighted
            else:
                atcaf_loss_raw = torch.tensor(0.0, device=device, dtype=torch.float32)







            # ç´¯ç§¯æŸå¤±ç»Ÿè®¡ (ä½¿ç”¨åŸå§‹AtCAFæŸå¤±è¿›è¡Œè®°å½•)
            running_pred_loss += pred_loss.item()
            running_atcaf_loss += atcaf_loss_raw.item() # åŸå§‹AtCAF
            running_atcaf_weighted_loss += atcaf_loss_weighted.item() # åŠ æƒAtCAFï¼ˆç”¨äºå¯¹è´¦ï¼‰
            running_total_loss += loss.item()
            batch_count += 1

            loss.backward()

            if not left_epochs:
                optimizer.step()
                left_epochs = update_epochs

        if not left_epochs:
            optimizer.step()





        # æ¯è½®è®­ç»ƒåè¯„ä¼°æ‰€æœ‰æ•°æ®é›†å¹¶æ‰“å°ç»“æœ
        train_results, _ = eval(model, metrics, train_data, device)
        valid_results, valid_loss = eval(model, metrics, valid_data, device)
        test_results, _ = eval(model, metrics, test_data, device)

        # æ‰“å°å®Œæ•´çš„è¯„ä¼°ç»“æœ
        print(f"\nğŸ“Š Epoch {epoch}/{total_epoch} å®Œæ•´è¯„ä¼°ç»“æœ:")
        print("=" * 80)

        # è®­ç»ƒé›†ç»“æœ
        print("ğŸ”µ è®­ç»ƒé›†ç»“æœ:")
        print(f"   Loss: {train_results['Loss']:.6f}")
        print(f"   MAE: {train_results['MAE']:.6f}")
        print(f"   Corr: {train_results['Corr']:.6f}")
        print(f"   Has0_acc_2: {train_results['Has0_acc_2']:.6f}")
        print(f"   Has0_F1_score: {train_results['Has0_F1_score']:.6f}")
        print(f"   Non0_acc_2: {train_results['Non0_acc_2']:.6f}")
        print(f"   Non0_F1_score: {train_results['Non0_F1_score']:.6f}")
        print(f"   Mult_acc_5: {train_results['Mult_acc_5']:.6f}")
        print(f"   Mult_acc_7: {train_results['Mult_acc_7']:.6f}")

        # éªŒè¯é›†ç»“æœ
        print("ğŸŸ¡ éªŒè¯é›†ç»“æœ:")
        print(f"   Loss: {valid_results['Loss']:.6f}")
        print(f"   MAE: {valid_results['MAE']:.6f}")
        print(f"   Corr: {valid_results['Corr']:.6f}")
        print(f"   Has0_acc_2: {valid_results['Has0_acc_2']:.6f}")
        print(f"   Has0_F1_score: {valid_results['Has0_F1_score']:.6f}")
        print(f"   Non0_acc_2: {valid_results['Non0_acc_2']:.6f}")
        print(f"   Non0_F1_score: {valid_results['Non0_F1_score']:.6f}")
        print(f"   Mult_acc_5: {valid_results['Mult_acc_5']:.6f}")
        print(f"   Mult_acc_7: {valid_results['Mult_acc_7']:.6f}")

        # æµ‹è¯•é›†ç»“æœ
        print("ğŸ”´ æµ‹è¯•é›†ç»“æœ:")
        print(f"   Loss: {test_results['Loss']:.6f}")
        print(f"   MAE: {test_results['MAE']:.6f}")
        print(f"   Corr: {test_results['Corr']:.6f}")
        print(f"   Has0_acc_2: {test_results['Has0_acc_2']:.6f}")
        print(f"   Has0_F1_score: {test_results['Has0_F1_score']:.6f}")
        print(f"   Non0_acc_2: {test_results['Non0_acc_2']:.6f}")
        print(f"   Non0_F1_score: {test_results['Non0_F1_score']:.6f}")
        print(f"   Mult_acc_5: {test_results['Mult_acc_5']:.6f}")
        print(f"   Mult_acc_7: {test_results['Mult_acc_7']:.6f}")

        print("=" * 80)

        # æ£€æŸ¥å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            best_epoch = epoch

            # åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if best_model_path and os.path.exists(best_model_path):
                os.remove(best_model_path)

            # ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹
            model_name = f'best_model_epoch_{epoch}_loss_{valid_loss:.4f}.pt'
            best_model_path = os.path.join(model.model_path, model_name)
            torch.save(model.state_dict(), best_model_path)

            print(f"\nâ­ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜! Epoch: {epoch}, æœ€ä½³éªŒè¯æŸå¤±: {valid_loss:.4f}")
            print(f"   æ¨¡å‹è·¯å¾„: {best_model_path}")

    # ========== è®­ç»ƒç»“æŸï¼ŒåŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼° ==========
    print("\n" + "="*50)
    print("ğŸ è®­ç»ƒå®Œæˆ! æ­£åœ¨åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    print("="*50)

    if best_model_path and os.path.exists(best_model_path):
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½åœ¨ç¬¬ {best_epoch} ä¸ªepochæ‰¾åˆ°çš„æœ€ä½³æ¨¡å‹...")
        print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_valid_loss:.4f}")
        print(f"   æ¨¡å‹è·¯å¾„: {best_model_path}")

        # åŠ è½½æœ€ä½³æ¨¡å‹çš„çŠ¶æ€å­—å…¸
        model.load_state_dict(torch.load(best_model_path))

        print("\nğŸ§ª ä½¿ç”¨æœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
        final_test_results, _ = eval(model, metrics, test_data, device)

        print("\n" + "="*50)
        print("ğŸ‰ åŸºäºæœ€ä½³éªŒè¯æŸå¤±çš„æœ€ç»ˆæµ‹è¯•ç»“æœ ğŸ‰")
        print(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {best_valid_loss:.4f} (ç¬¬ {best_epoch} ä¸ªepoch)")
        print(f"  - MAE: {final_test_results['MAE']:.4f}")
        print(f"  - Corr: {final_test_results['Corr']:.4f}")
        print(f"  - Accuracy-7: {final_test_results['Mult_acc_7']:.4f}")
        print(f"  - Accuracy-2 (Has0): {final_test_results['Has0_acc_2']:.4f}")
        print(f"  - F1-Score (Has0): {final_test_results['Has0_F1_score']:.4f}")
        print(f"  - Accuracy-2 (Non0): {final_test_results['Non0_acc_2']:.4f}")
        print(f"  - F1-Score (Non0): {final_test_results['Non0_F1_score']:.4f}")
        print("="*50)
    else:
        print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æœ€ä½³æ¨¡å‹ï¼Œæ— æ³•è¿›è¡Œæœ€ç»ˆæµ‹è¯•ã€‚")


def eval(model, metrics, data, device):
    model.eval()
    with torch.no_grad():
        pred, truth = [], []
        loss = 0
        lens = 0
        for batch_data in data:
            text = batch_data['raw_text']
            vision = batch_data['vision'].clone().detach().to(device).float()
            audio = batch_data['audio'].clone().detach().to(device).float()
            label = batch_data['labels']['M'].clone().detach().view(-1).to(device).float()
            _pred, _ = model(text, vision, audio, mode='test')
            pred.append(_pred.view(-1))
            truth.append(label)
            _loss = torch.mean((label-_pred)*(label-_pred))
            loss += _loss.item() * len(label)
            lens += len(label)
        pred = torch.cat(pred).to(torch.device('cpu'), ).squeeze()
        truth = torch.cat(truth).to(torch.device('cpu')).squeeze()
        eval_results = metrics.eval_mosei_regression(truth, pred)
        eval_results['Loss'] = loss / lens
    model.train()
    return eval_results, loss / lens


def TVA_test_fusion(config, metric, test_data):

    seed = config.seed
    log_path = config.LOGPATH + "MOSI_TVA_Test." + datetime.datetime.now().strftime('%Y-%m-%d-%H%M%S') + '.log'

    write_config(config, log_path)

    model = TVA_fusion(config=config)

    device = config.DEVICE
    model.to(device)

    model.load_model()
    result, loss = eval(model,metric, test_data, device)

    log = '\nTVA_Test\n\tHas0_acc_2:%s\n\tHas0_F1_score:%s\n\tNon0_acc_2:%s\n\t' \
        'Non0_F1_score:%s\n\tMult_acc_5:%s\n\tMult_acc_7:%s\n\tMAE:%s\n\tCorr:%s\n\tLoss:%s\n' \
        '------------------------------------------' % (
        result['Has0_acc_2'], result['Has0_F1_score'], result['Non0_acc_2'], result['Non0_F1_score'],
        result['Mult_acc_5'], result['Mult_acc_7'], result['MAE'], result['Corr'], loss
    )
    print(log)
    write_log(log, log_path)

    return result

